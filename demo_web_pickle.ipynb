{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:59:14.694742Z",
     "start_time": "2018-03-19T10:59:06.388864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utk/.virtualenvs/jupjup/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:01:15.276589Z",
     "start_time": "2018-03-19T11:01:07.650641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/utk/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:00:17.819096Z",
     "start_time": "2018-03-19T11:00:17.751397Z"
    }
   },
   "outputs": [],
   "source": [
    "# готовим ТРЕНИРОВОЧНЫЕ данные и метки для них\n",
    "data_train = pd.read_csv('products_sentiment_train.tsv', header=None, sep='\\t')\n",
    "# data_train.head()\n",
    "texts_train = data_train[0].values\n",
    "labels = data_train[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:01:21.264616Z",
     "start_time": "2018-03-19T11:01:19.080181Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english') # подключаем английский словарь для Стемминга\n",
    "wnl = WordNetLemmatizer() # лемматизация\n",
    "\n",
    "texts_train2 = []\n",
    "\n",
    "# лемматизация \n",
    "for review in texts_train:\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    texts_train2.append([wnl.lemmatize(token) for token in tokens]) # лемматизация\n",
    "    \n",
    "# сшиваем строки в список\n",
    "texts_train2 = list(map(lambda x: ' '.join(x), texts_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:06:25.323494Z",
     "start_time": "2018-03-19T11:06:12.024165Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Векторайзер и модель\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range=(3, 16), analyzer=u'char')\n",
    "mx = tf_vectorizer.fit_transform(texts_train2)\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(mx, labels)\n",
    "\n",
    "# сериализация векторайзера и модели в бинарные файлы\n",
    "with open('./DefaultLogisticBigramUnprocessedTextSentiment.pkl', 'wb') as handle:\n",
    "    pickle.dump(model, handle)\n",
    "with open('./BigramUnprocessedVectorizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tf_vectorizer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:19:07.215552Z",
     "start_time": "2018-03-19T11:19:07.161699Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MySentimentClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f1d22155f820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_sentiment_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMySentimentClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcodecs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MySentimentClassifier'"
     ]
    }
   ],
   "source": [
    "from my_sentiment_classifier import MySentimentClassifier\n",
    "from codecs import open\n",
    "import time\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "print (\"Preparing classifier\")\n",
    "start_time = time.time()\n",
    "classifier = MySentimentClassifier()\n",
    "print (\"Classifier is ready\")\n",
    "print (time.time() - start_time, \"seconds\")\n",
    "\n",
    "@app.route(\"/\", methods=[\"POST\", \"GET\"])\n",
    "def index_page(text=\"\", prediction_message=\"\"):\n",
    "    if request.method == \"POST\":\n",
    "        text = request.form[\"text\"]\n",
    "        print (text)\n",
    "        prediction_message = classifier.get_prediction_message(text)\n",
    "        print (prediction_message)\n",
    "    return render_template('hello.html', text=text, prediction_message=prediction_message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
